{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import bayes_pytorch_local_reparam as bayes\n",
    "\n",
    "import torch\n",
    "from torch import Tensor as T\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 1\n",
    "\n",
    "\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = DataLoader(test_set, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    test_accuracy = 0\n",
    "    n_test_samples = 0\n",
    "    model.eval()\n",
    "    for test_batch_idx, (test_data, test_target) in enumerate(test_loader):\n",
    "        test_data, test_target = test_data.cuda(), test_target.cuda()\n",
    "        test_data = autograd.Variable(test_data, volatile=True)\n",
    "        output = model(test_data).data\n",
    "        _, argmax = output.max(1)\n",
    "        test_accuracy += test_target.eq(argmax).sum()\n",
    "        n_test_samples += test_target.size(0)\n",
    "\n",
    "    test_accuracy /= n_test_samples\n",
    "    return test_accuracy\n",
    "    \n",
    "    \n",
    "def save(model, path):\n",
    "    print('Saving..')\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    " \n",
    "    state = model.state_dict()\n",
    "    torch.save(state, 'checkpoint/{}.t7'.format(path))\n",
    "    \n",
    "    \n",
    "# load model\n",
    "def load_model(basic_model, path):\n",
    "    checkpoint = torch.load('./checkpoint/{}.t7'.format(path))\n",
    "    basic_model.load_state_dict(checkpoint)\n",
    "    basic_model.cuda()\n",
    "    \n",
    "    \n",
    "def train(train_loader, test_loader, model, optimizer, scheduler=None, start_epoch=0, stop_epoch=20):\n",
    "\n",
    "    from logger import Logger\n",
    "    logger = Logger('./logs')\n",
    "\n",
    "    n_epochs = stop_epoch - start_epoch\n",
    "    n_steps = 0\n",
    "    for epoch in tqdm_notebook(range(start_epoch, stop_epoch), desc='epochs', total=n_epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        n_train_samples = 0\n",
    "        n_train_batches = 0\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        for data, target in tqdm_notebook(train_loader, leave=False):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            data, target = autograd.Variable(data), autograd.Variable(target)           \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#             output = 0\n",
    "#             for j in range(5):\n",
    "#                 output += model(data)\n",
    "#             output /= 5.0\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "            # (2) Log values and gradients of the parameters (histogram)\n",
    "            if n_train_batches % 100 == 1:\n",
    "                for tag, value in model.named_parameters():\n",
    "                    if 'logsigma' in tag or '_mu' in tag:\n",
    "                        \n",
    "                        if 'logsigma' in tag:\n",
    "                            v = torch.exp(value)\n",
    "                            tag = tag.replace('logsigma', 'sigma')\n",
    "                        else:\n",
    "                            v = value\n",
    "                        \n",
    "                        tag = tag.replace('.', '/')\n",
    "                        logger.histo_summary(tag, v.data.cpu().numpy(), epoch)\n",
    "                        if value.requires_grad:\n",
    "                            logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), epoch)\n",
    "        \n",
    "            train_loss += loss.data[0]\n",
    "            n_train_samples += target.size(0)\n",
    "            n_train_batches += 1\n",
    "            n_steps += 1\n",
    "            \n",
    "            logger.scalar_summary('loss', loss.data[0], n_steps)\n",
    "            \n",
    "        train_loss /= n_train_batches\n",
    "        \n",
    "        \n",
    "        \n",
    "        # evaluate\n",
    "        test_accuracy = evaluate(model, test_loader)\n",
    "        \n",
    "        # print progress\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTest acc: {:.3f}'.format(\n",
    "              epoch, n_train_samples, len(train_loader.dataset),\n",
    "              100. * n_train_batches / len(train_loader), train_loss,\n",
    "              test_accuracy))\n",
    "        \n",
    "        #============ TensorBoard logging ============#\n",
    "        # (1) Log the scalar values\n",
    "        info = {\n",
    "            'accuracy': test_accuracy\n",
    "        }\n",
    "\n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag, value, epoch)\n",
    "        \n",
    "        \n",
    "def lr_lambda(epoch):\n",
    "    gamma = 1\n",
    "    if epoch > 0 and epoch < 150:\n",
    "        gamma = 1\n",
    "    elif epoch >= 150 and epoch < 250:\n",
    "        gamma = 0.1\n",
    "    elif epoch >= 250 and epoch < 350:\n",
    "        gamma = 0.01\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet101 = bayes.ResNet(bayes.BasicBlock, [3, 4, 23, 3], random_weights=False)\n",
    "resnet101.cuda()\n",
    "\n",
    "parameters = [p for p in resnet101.parameters() if p.requires_grad]\n",
    "# optimizer = optim.Adam(parameters, lr=0.01)\n",
    "\n",
    "optimizer = optim.SGD(parameters, lr=0.1, momentum=0.9, weight_decay=5e-4)   \n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "train(train_loader, test_loader, resnet101, optimizer, scheduler=scheduler, n_epochs=350)\n",
    "save(resnet101, 'resnet101_epoch=350')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet101_loaded = bayes.ResNet(bayes.BasicBlock, [3, 4, 23, 3], random_weights=False)\n",
    "load_model(resnet101_loaded, 'resnet101_epoch=350')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet with Random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resnet_random = bayes.ResNet(bayes.RandomBasicBlock, [3, 4, 23, 3], random_weights=True)\n",
    "# resnet_random.cuda()\n",
    "\n",
    "resnet_random = bayes.ResNet(bayes.RandomBasicBlock, [2, 2, 2, 2], random_weights=True)\n",
    "resnet_random.cuda()\n",
    "\n",
    "\n",
    "parameters = [p for p in resnet_random.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(parameters, lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "# optimizer = optim.Adam(parameters, lr=0.0001)\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix all conv layers parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, p in resnet_random.named_parameters():\n",
    "    if 'W_mu' in name or 'W_logstd' in name:\n",
    "        p.requires_grad = False\n",
    "        \n",
    "parameters = [p for p in resnet_random.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(parameters, lr=0.01, momentum=0.9, nesterov=True)\n",
    "# optimizer = optim.Adam(parameters, lr=0.0001)\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab647146d4db43c385efc2c05b07b5b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19b6b70249f433e8696cc8acfa79dc9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [50000/50000 (100%)]\tLoss: 2.280023\tTest acc: 0.131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11503d9d5e604c1ca509452e517cabd9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [50000/50000 (100%)]\tLoss: 2.267630\tTest acc: 0.149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff8d62fc7834b47b023f3fad3c2a046"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [50000/50000 (100%)]\tLoss: 2.263999\tTest acc: 0.159\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632e64bcbc724aab8a55d3ce57a54609"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [50000/50000 (100%)]\tLoss: 2.262529\tTest acc: 0.159\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f6c1ffb0244cc7925fcbb8191c5113"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-18:\n",
      "KeyboardInterrupt\n",
      "Process Process-17:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/yeahrmek/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fa922309a519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-9b6f96140793>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, test_loader, model, optimizer, scheduler, start_epoch, stop_epoch)\u001b[0m\n\u001b[1;32m     79\u001b[0m                             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhisto_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/grad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mn_train_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mn_train_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_loader, test_loader, resnet_random, optimizer, start_epoch=0, stop_epoch=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracy = evaluate(resnet101_random, test_loader)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.sigmoid(torch.exp(resnet101_random.conv1.W_logstd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(resnet101_random, 'resnet101_random_epoch=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(model, test_loader):\n",
    "    test_accuracy = 0\n",
    "    n_test_samples = 0\n",
    "    model.eval()\n",
    "    for test_batch_idx, (test_data, test_target) in enumerate(test_loader):\n",
    "        test_data, test_target = test_data.cuda(), test_target.cuda()\n",
    "        test_data = autograd.Variable(test_data, volatile=True)\n",
    "        output = model(test_data).data\n",
    "        _, argmax = output.max(1)\n",
    "        test_accuracy += test_target.eq(argmax).sum()\n",
    "        n_test_samples += test_data.size()[0]\n",
    "    print('tTest acc: {:.3f}'.format(test_accuracy / n_test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet101_random_loaded = bayes.ResNet(bayes.RandomBasicBlock, [3, 4, 23, 3], random_weights=True)\n",
    "\n",
    "load_model(resnet101_random_loaded, 'resnet101_random_epoch=350')\n",
    "test(resnet101_random_loaded, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test(resnet101_random, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet101 = bayes.ResNet(bayes.BasicBlock, [3, 4, 23, 3], random_weights=False)\n",
    "\n",
    "load_model(resnet101, 'resnet101_epoch=350')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, p in resnet101_random.named_parameters():\n",
    "    if 'W_logstd' in name:\n",
    "        std = torch.exp(p).data.cpu().numpy()[0]\n",
    "        print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, p in resnet101.named_parameters():\n",
    "    print(name)\n",
    "    if 'weight' in name:\n",
    "        std = p.std().data.cpu().numpy()[0]\n",
    "        print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
